{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import socket\n",
    "import numpy as np\n",
    "from cv2 import aruco\n",
    "from ikpy.chain import Chain\n",
    "from ikpy.urdf import URDF\n",
    "from ikpy.utils import geometry\n",
    "import numpy as np\n",
    "\n",
    "camera_index = 1\n",
    "ver_angle_impact = 0.1\n",
    "contraction = 1\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"ikpy.chain\")\n",
    "# Step 1: Function to calculate pixel distance along the y-axis between Marker 1 at origin and Marker 2\n",
    "def calculate_pixel_distance(marker_y_axis_y):\n",
    "    # Since Marker 1 is at (0,0), the pixel distance is simply the y-coordinate of Marker 2\n",
    "    pixel_distance = abs(marker_y_axis_y)\n",
    "    return pixel_distance\n",
    "\n",
    "# Step 2: Function to adjust pixel distance with cos(phi)\n",
    "def adjust_distance_with_angle(pixel_distance, angle):\n",
    "    # Adjust pixel distance using cos(phi) to account for angle distortion\n",
    "    adjusted_distance = pixel_distance * np.cos(angle)\n",
    "    return adjusted_distance\n",
    "\n",
    "# Step 3: Get robot's coordinate system for origin (O) and reference point (R)\n",
    "def get_robot_coordinates():\n",
    "    print(\"Enter the coordinates of the camera origin in the robot's coordinate system (Ox, Oy):\")\n",
    "    Ox, Oy = map(float, input(\"Ox Oy: \").split())\n",
    "    print(\"Enter the coordinates of the reference point in the robot's coordinate system (Rx, Ry):\")\n",
    "    Rx, Ry = map(float, input(\"Rx Ry: \").split())\n",
    "    return (Ox, Oy), (Rx, Ry)\n",
    "\n",
    "# Step 4: Calculate the proportionality constant c\n",
    "def calculate_proportionality_constant(robot_distance, pixel_distance):\n",
    "    # c = d / p\n",
    "    c = robot_distance / pixel_distance\n",
    "    return c\n",
    "\n",
    "# Step 5: Calculate theta, the angle between robot's y-axis and camera's y-axis\n",
    "def calculate_theta(Ox, Oy, Rx, Ry):\n",
    "    # Calculate the angle using atan2 for correct quadrant handling\n",
    "    delta_x = Rx - Ox\n",
    "    delta_y = Ry - Oy\n",
    "    theta = np.arctan2(delta_y, delta_x)\n",
    "    return theta-np.pi/2\n",
    "\n",
    "# Step 6: Transformation function to convert any point (x, y) in camera coordinates to robot coordinates\n",
    "def camera_to_robot_coordinates(O, c, theta, point_in_camera):\n",
    "    # Ensure input types are correct\n",
    "    O = np.array(O, dtype=np.float64)  # Origin of robot coordinates\n",
    "    point_in_camera = np.array(tuple(contraction * x for x in point_in_camera), dtype=np.float64)  # Camera coordinates\n",
    "\n",
    "    # Rotation matrix\n",
    "    rotation_matrix = np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta), np.cos(theta)]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "    # Scale and rotate the point\n",
    "    adjusted_point = c * np.dot(rotation_matrix, point_in_camera)\n",
    "\n",
    "    # Translate to the robot's coordinate system by adding origin coordinates (Ox, Oy)\n",
    "    point_in_robot = O + adjusted_point\n",
    "\n",
    "    return point_in_robot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Start the video capture loop\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFailed to capture image\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the camera\n",
    "cap = cv2.VideoCapture(camera_index)  # Change to 1 or 2 if multiple cameras are connected\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera. Check permissions or try a different index.\")\n",
    "    exit()\n",
    "\n",
    "# Define the dictionary and parameters for ArUco detection\n",
    "aruco_dict = aruco.getPredefinedDictionary(aruco.DICT_6X6_250)\n",
    "parameters = aruco.DetectorParameters()\n",
    "parameters.minMarkerPerimeterRate = 0.01  # Increase sensitivity for smaller markers\n",
    "parameters.adaptiveThreshWinSizeMin = 3   # Min window size for adaptive thresholding\n",
    "parameters.adaptiveThreshWinSizeMax = 25  # Max window size for adaptive thresholding\n",
    "parameters.adaptiveThreshWinSizeStep = 5  # Step size for adaptive thresholding\n",
    "parameters.polygonalApproxAccuracyRate = 0.05  # Improves marker detection near edges\n",
    " \n",
    "# Get the frame resolution to determine camera center\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to capture initial image for resolution check.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "height, width = frame.shape[:2]\n",
    "camera_center = (width // 2, int(height * 4.5 / 5))\n",
    "\n",
    "\n",
    "# Dummy camera parameters (required for pose estimation)\n",
    "camera_matrix = np.array([[800, 0, width // 2], [0, 800, height // 2], [0, 0, 1]], dtype=float)\n",
    "dist_coeffs = np.zeros((4, 1))\n",
    "\n",
    "# Dictionary to store marker data dynamically\n",
    "marker_data = {}\n",
    "\n",
    "# Start the video capture loop\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    # Create a separate grayscale frame for clean marker detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect markers on the clean grayscale frame\n",
    "    corners, ids, rejected = aruco.detectMarkers(gray, aruco_dict, parameters=parameters)\n",
    "\n",
    "    # Overlay for drawing axes and detected marker information\n",
    "    overlay = frame.copy()\n",
    "    alpha = 0.5  # Transparency factor for overlay\n",
    "\n",
    "    # Draw semi-transparent axes on the overlay\n",
    "    cv2.line(overlay, (0, camera_center[1]), (width, camera_center[1]), (255, 0, 0), 3)  # x-axis\n",
    "    cv2.line(overlay, (camera_center[0], 0), (camera_center[0], height), (0, 255, 0), 3)  # y-axis\n",
    "\n",
    "    # If markers are detected\n",
    "    if ids is not None:\n",
    "        for i, corner in enumerate(corners):\n",
    "            # Estimate pose of the marker to get its rotation and translation vectors\n",
    "            rvec, tvec, _ = aruco.estimatePoseSingleMarkers(corner, 0.05, camera_matrix, dist_coeffs)\n",
    "\n",
    "            # Convert rotation vector to rotation matrix\n",
    "            rotation_matrix, _ = cv2.Rodrigues(rvec[0])\n",
    "\n",
    "            # Extract pitch and roll, adding a small epsilon to avoid division by zero\n",
    "            pitch = math.degrees(math.atan2(-rotation_matrix[2, 0], np.sqrt(rotation_matrix[2, 1]**2 + rotation_matrix[2, 2]**2 + 1e-6)))\n",
    "            roll = math.degrees(math.atan2(rotation_matrix[2, 1], rotation_matrix[2, 2]))\n",
    "\n",
    "            # Calculate the vertical angle (angle between marker surface normal and camera viewing vector)\n",
    "            marker_normal = rotation_matrix[:, 2]  # Z-axis of the marker\n",
    "            camera_viewing_vector = np.array([0, 0, -1])\n",
    "            cos_theta = np.dot(marker_normal, camera_viewing_vector) / (np.linalg.norm(marker_normal) * np.linalg.norm(camera_viewing_vector))\n",
    "            vertical_angle = np.degrees(np.arccos(cos_theta))\n",
    "\n",
    "            # Calculate the center of the marker relative to the camera center\n",
    "            center_x = int(np.mean(corner[0][:, 0]))\n",
    "            center_y = int(np.mean(corner[0][:, 1]))\n",
    "            \n",
    "            relative_x = (center_x - camera_center[0])\n",
    "            relative_y = (camera_center[1] - center_y)  # Inverting y-axis for upward positive direction\n",
    "\n",
    "            # Draw detected markers on the overlay\n",
    "            aruco.drawDetectedMarkers(overlay, [corner], ids[i])\n",
    "\n",
    "            # Draw the coordinate axes on the marker\n",
    "            cv2.drawFrameAxes(overlay, camera_matrix, dist_coeffs, rvec[0], tvec[0], 0.1)  # Axis length can be adjusted\n",
    "\n",
    "            # Text information with different colors and larger font size\n",
    "            info_texts = [\n",
    "                (\"Pitch: {:.2f}°\".format(pitch), (255, 255, 0)),   # Yellow for Pitch\n",
    "                (\"Roll: {:.2f}°\".format(roll), (0, 0, 255)),       # Red for Roll\n",
    "                (\"Vertical Angle: {:.2f}°\".format(vertical_angle), (0, 255, 255)),  # Cyan for Vertical Angle\n",
    "                (\"Pos: ({}, {})\".format(relative_x, relative_y), (255, 255, 255))  # White for Position\n",
    "            ]\n",
    "\n",
    "            # Determine the position of the box to avoid covering the marker\n",
    "            box_width, box_height = 250, 150\n",
    "            box_x = center_x + 20 if center_x < width - box_width - 20 else center_x - box_width - 20\n",
    "            box_y = center_y + 20 if center_y < height - box_height - 20 else center_y - box_height - 20\n",
    "\n",
    "            # Draw the opaque black background box for information\n",
    "            cv2.rectangle(overlay, (box_x, box_y), (box_x + box_width, box_y + box_height), (0, 0, 0), -1)\n",
    "\n",
    "            # Display all information in the box with specified colors and larger font size\n",
    "            for j, (text, color) in enumerate(info_texts):\n",
    "                cv2.putText(overlay, text, (box_x + 10, box_y + 35 + j * 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "\n",
    "            # Store the marker data dynamically in the dictionary\n",
    "            marker_id = ids[i][0]\n",
    "            marker_data[marker_id] = {\n",
    "                \"pitch\": pitch,\n",
    "                \"roll\": roll,\n",
    "                \"vertical_angle\": vertical_angle * ver_angle_impact,\n",
    "                \"position\": (relative_x, relative_y)\n",
    "            }\n",
    "\n",
    "    # Blend the overlay with the original frame to apply semi-transparent axes and marker info\n",
    "    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "    # Display the frame with overlay\n",
    "    cv2.imshow('ArUco Marker Detection with Orientation', frame)\n",
    "\n",
    "    # Capture marker data on pressing 'c'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('c'):\n",
    "        # Print captured marker data and find the maximum y-coordinate and vertical angle\n",
    "        print(\"\\n\\nCaptured marker data:\")\n",
    "\n",
    "        RCy = -float('inf')\n",
    "        max_marker_id = None\n",
    "        max_position = None\n",
    "        vertical_angle_for_max_y = None  # To store vertical angle for the max y marker\n",
    "\n",
    "        for marker_id, data in marker_data.items():\n",
    "            print(f\"Marker ID: {marker_id}, Data: {data}\")\n",
    "\n",
    "            # Extract the y-coordinate from the position\n",
    "            relative_y = data[\"position\"][1]\n",
    "            if relative_y > RCy:\n",
    "                RCy = relative_y\n",
    "                max_marker_id = marker_id\n",
    "                max_position = data[\"position\"]\n",
    "                vertical_angle_for_max_y = data[\"vertical_angle\"]  # Store vertical angle\n",
    "\n",
    "        # Print the maximum y-coordinate and corresponding marker information\n",
    "        print(f\"Maximum y-coordinate (RCy): {RCy} for Marker ID: {max_marker_id} at Position: {max_position}\")\n",
    "        print(f\"Vertical Angle for Marker with Max Y: {vertical_angle_for_max_y}\")\n",
    "        marker_y_axis_y = RCy\n",
    "\n",
    "        user_input = input(\"\\nDo you want to go ahead? (y/n): \").strip().lower()\n",
    "\n",
    "        if user_input == \"y\":\n",
    "                print(\"\\nProceeding to send_angles to the ROBOT!\\n\\n\")\n",
    "                break\n",
    "                # Add code to go ahead with the action here\n",
    "        elif user_input == \"n\":\n",
    "                print(\"Stopping.\\n\")\n",
    "                continue  # Exit the loop if they choose not to go ahead\n",
    "        else:\n",
    "                print(\"Invalid input, please answer with 'yes' or 'no'.\")\n",
    "                continue\n",
    "        \n",
    "    # Exit on pressing 'q'\n",
    "    elif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# 1. Calculate pixel distance between the origin and the y-axis marker\n",
    "pixel_distance = calculate_pixel_distance(marker_y_axis_y)\n",
    "print(\"Pixel distance between markers:\", pixel_distance)\n",
    "\n",
    "# 2. Adjust for angle with cos(phi)\n",
    "phi = np.radians(vertical_angle_for_max_y)  # Example angle (in radians) between marker's axis and camera line of sight\n",
    "adjusted_pixel_distance = adjust_distance_with_angle(pixel_distance, phi) #adjust_distance_with_angle(pixel_distance, phi)\n",
    "print(\"Adjusted pixel distance:\", adjusted_pixel_distance,\"\\n\")\n",
    "\n",
    "# 3. Get robot's coordinates for origin (O) and reference (R)\n",
    "while True:\n",
    "    try:\n",
    "        O, R = get_robot_coordinates()\n",
    "        break\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"Camera Origin in robot's coordinate system: O(0,0)== {O}\")\n",
    "print(f\"Camera Reference in robot's coordinate system: R(0,{marker_y_axis_y})== {R}\\n\")\n",
    "\n",
    "# 4. Calculate the robot distance between O and R\n",
    "robot_distance = np.linalg.norm(np.array(R) - np.array(O))\n",
    "print(f\"Distance between points (robot_distance): {robot_distance}\")\n",
    "\n",
    "# 5. Calculate proportionality constant c\n",
    "c = calculate_proportionality_constant(robot_distance, adjusted_pixel_distance)\n",
    "print(\"Proportionality constant (c):\", c)\n",
    "\n",
    "# 6. Calculate theta, the angle between robot's y-axis and camera's y-axis\n",
    "theta = calculate_theta(*O, *R)\n",
    "print(\"Angle theta (radians) between robot's y-axis and camera's y-axis:\", theta*(180/np.pi))\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "# Function to handle rectangle selection and image processing\n",
    "def process_image(image):\n",
    "    # Let the user select a rectangular area\n",
    "    r = cv2.selectROI(\"Select the area to keep\", image)\n",
    "    cv2.destroyWindow(\"Select the area to keep\")\n",
    "    \n",
    "    # Create a white background image\n",
    "    output_image = np.ones_like(image) * 255\n",
    "    \n",
    "    # Copy the selected rectangle region from the original image to the white background\n",
    "    x, y, w, h = r\n",
    "    output_image[y:y+h, x:x+w] = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Save the processed image\n",
    "    cv2.imwrite(\"captured_img.png\", output_image)\n",
    "    \n",
    "    # Show the processed image\n",
    "    cv2.imshow(\"Processed Image\", output_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Solves the maze\n",
    "def solve_image(img):\n",
    "    # Convert the image to grayscale (black and white)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply a binary threshold to get a pure black and white image (invert the colors)\n",
    "    ret, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV) \n",
    "    # Increase the kernel size for stronger blurring\n",
    "    blurred = cv2.GaussianBlur(gray, (15, 15), 0)\n",
    "    cv2.imshow('Threshold 1', thresh)  # Show the first thresholded image\n",
    "\n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    dc = cv2.drawContours(thresh.copy(), contours, 0, (0, 0, 255), 10)  # Red color with thicker lines\n",
    "    cv2.imshow('Contours 1', dc)  # Show the first contours\n",
    "    \n",
    "    # Apply second thresholding to clean up the contours\n",
    "    ret, thresh = cv2.threshold(dc, 240, 255, cv2.THRESH_BINARY)\n",
    "    #cv2.imshow('Threshold 2', thresh)  # Show the second thresholded image\n",
    "\n",
    "    # Dilation\n",
    "    # Kernel size for dilation and erosion\n",
    "    ke = kernel_defined\n",
    "    kernel = np.ones((ke, ke), np.uint8)\n",
    "    dilation = cv2.dilate(thresh, kernel, iterations=1)\n",
    "    #cv2.imshow('Dilation', dilation)  # Show dilation result\n",
    "\n",
    "    # Erosion\n",
    "    erosion = cv2.erode(dilation, kernel, iterations=1)\n",
    "    #cv2.imshow('Erosion', erosion)  # Show erosion result\n",
    "\n",
    "    # Find differences between dilation and erosion\n",
    "    diff = cv2.absdiff(dilation, erosion)\n",
    "    #cv2.imshow('Difference', diff)  # Show the difference between dilation and erosion\n",
    "\n",
    "    # Erosion the diff image again\n",
    "    ke1 = int(round(ke / 2.5, 0))  # Convert to integer explicitly\n",
    "    kernel = np.ones((ke1, ke1), np.uint8)\n",
    "    erosion_diff = cv2.erode(diff, kernel, iterations=1)\n",
    "    #cv2.imshow('Erosion Diff', erosion_diff)  # Show the eroded difference image\n",
    "\n",
    "    # Invert the mask\n",
    "    mask_inv = cv2.bitwise_not(erosion_diff)\n",
    "    cv2.imshow('Mask', mask_inv)  # Show the inverted mask\n",
    "\n",
    "    # Split the channels of the maze image\n",
    "    b, g, r = cv2.split(img)\n",
    "\n",
    "    # Mask out the green and red color from the solved path\n",
    "    r = cv2.bitwise_and(r, r, mask=mask_inv)\n",
    "\n",
    "    b = cv2.bitwise_and(b, b, mask=mask_inv)\n",
    "\n",
    "    # Merge the channels to display the solved maze\n",
    "    res = cv2.merge((b, g, r))\n",
    "    cv2.imshow('Solved Maze', res)  # Show the solved maze\n",
    "\n",
    "    cv2.waitKey(6000)  # Wait for a key press\n",
    "    # Close all windows after any key is pressed\n",
    "    cv2.destroyAllWindows()\n",
    "    return mask_inv\n",
    "\n",
    "# Opens the camera to capture the maze\n",
    "def open_camera():\n",
    "    # Capture the live video feed\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Show the live video feed\n",
    "        cv2.imshow('Live Feed', frame)\n",
    "        \n",
    "        # Wait for keypress: 'c' to capture image, 'q' to quit\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('c'):\n",
    "            cv2.destroyWindow(\"Live Feed\")\n",
    "            captured_image = frame.copy()\n",
    "            #cv2.imshow(\"Captured Image\", captured_image)\n",
    "            cv2.waitKey(1)  # Wait a moment for the user to see the captured image\n",
    "            return captured_image\n",
    "        elif key == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Marks coordinates and orders them to cross the maze properly\n",
    "def process_solution(filename, direction=-1, start_from_point=1):\n",
    "    # Load the image\n",
    "    img = cv2.imread(filename + '.png')\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Gaussian blur for noise reduction\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "    # Apply binary threshold to isolate the black line/loop\n",
    "    _, binary = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Skeletonize using OpenCV's morphological operations\n",
    "    skeleton = np.zeros_like(binary)\n",
    "    element = cv2.getStructuringElement(cv2.MORPH_CROSS, (3, 3))\n",
    "    temp_binary = binary.copy()\n",
    "\n",
    "    while cv2.countNonZero(temp_binary) > 0:\n",
    "        eroded = cv2.erode(temp_binary, element)\n",
    "        temp = cv2.dilate(eroded, element)\n",
    "        temp = cv2.subtract(temp_binary, temp)\n",
    "        skeleton = cv2.bitwise_or(skeleton, temp)\n",
    "        temp_binary = eroded\n",
    "\n",
    "    # Convert skeleton to contours for further processing\n",
    "    contours, _ = cv2.findContours(skeleton, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Focus on the largest contour (assuming it's the main loop)\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Approximate the contour to reduce noise and focus on corners\n",
    "    epsilon = 0.01 * cv2.arcLength(contour, True)  # Adjust epsilon for more/less simplification\n",
    "    approx = cv2.approxPolyDP(contour, epsilon, True)\n",
    "\n",
    "    # Convert to a more uniform format (list of (x, y) tuples)\n",
    "    approx = [tuple(point[0]) for point in approx]\n",
    "\n",
    "    # Find the top-left corner to start numbering\n",
    "    top_left_corner = min(approx, key=lambda p: (p[0] + p[1]))  # Smallest (x + y) is the top-left corner\n",
    "    start_index = approx.index(top_left_corner)\n",
    "\n",
    "    # Reorder corners to start from the top-left corner\n",
    "    ordered_corners = approx[start_index:] + approx[:start_index]\n",
    "\n",
    "    # Adjust the start point (convert from 1-based index to 0-based index)\n",
    "    start_index = start_from_point - 1\n",
    "\n",
    "    # Reorder corners to start from the specified point\n",
    "    ordered_corners = ordered_corners[start_index:] + ordered_corners[:start_index]\n",
    "\n",
    "    # Reverse the order of points based on direction\n",
    "    if direction == -1:\n",
    "        ordered_corners = ordered_corners[::-1]\n",
    "\n",
    "    # Prepare the corner dictionary\n",
    "    corner_dict = {}\n",
    "\n",
    "    # Image size for coordinate system\n",
    "    height, width = skeleton.shape\n",
    "    center_x, center_y = width // 2, int(height * 4.5 / 5)\n",
    "\n",
    "    img_with_corners = cv2.cvtColor(skeleton, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    for i, (x, y) in enumerate(ordered_corners, 1):\n",
    "        # Adjust the coordinates to have the origin at the center\n",
    "        adjusted_x = x - center_x\n",
    "        adjusted_y = -(y - center_y)  # Flip y to make it positive upwards\n",
    "\n",
    "        # Draw the corner on the image\n",
    "        cv2.circle(img_with_corners, (x, y), 5, (0, 0, 255), -1)  # Red dot\n",
    "        # Annotate the corner with a serial number\n",
    "        cv2.putText(img_with_corners, f'{i}', (x + 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)\n",
    "        # Annotate the adjusted coordinates (in the center-based coordinate system)\n",
    "        cv2.putText(img_with_corners, f'({adjusted_x}, {adjusted_y})', (x + 5, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)\n",
    "        \n",
    "        # Add the point with adjusted coordinates to the dictionary as 'coords'\n",
    "        corner_dict[f\"Serial_{i}\"] = [adjusted_x, adjusted_y]\n",
    "\n",
    "    # Overlay the coordinate axes (x-axis and y-axis)\n",
    "    axis_color = (255, 0, 0)  # Blue for the axes\n",
    "    thickness = 2\n",
    "\n",
    "    # Draw x-axis\n",
    "    cv2.line(img_with_corners, (0, center_y), (width, center_y), axis_color, thickness)\n",
    "\n",
    "    # Draw y-axis\n",
    "    cv2.line(img_with_corners, (center_x, 0), (center_x, height), axis_color, thickness)\n",
    "\n",
    "    # Display the image with the points, their serial numbers, and the axes\n",
    "    cv2.imshow('Simplified Curve with Corners and Axes', img_with_corners)\n",
    "    cv2.imwrite(\"final_Corners_With_Axes.png\", img_with_corners)\n",
    "    print(\"Saved the solution image into \\\"final_Corners_With_Axes.png\\\"\")\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return corner_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This captures the image\n",
    "cap = cv2.VideoCapture(camera_index)\n",
    "captured_image = open_camera()\n",
    "if captured_image is not None:\n",
    "    # Process the captured image\n",
    "    process_image(captured_image)\n",
    "time.sleep(1)\n",
    "cap.release() \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maze solution\n",
    "kernel_defined =70 # Change this for better maze solution detection\n",
    "\n",
    "# This finds solution\n",
    "img = cv2.imread('captured_img.png')\n",
    "solution = solve_image(img)\n",
    "cv2.imshow('Final', solution)\n",
    "cv2.waitKey(2000000)\n",
    "cv2.imwrite(\"solution.png\", solution)\n",
    "print(\"Saved the solution image into \\\"solution.png\\\"\")\n",
    "\n",
    " # Releases the video capture object\n",
    "cv2.destroyAllWindows()  # Closes any open OpenCV windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = (-384.83, -462.958)\n",
    "c= 0.44737345224705083\n",
    "theta = 2.642136069702122 * np.pi/180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This marks coordinates and orders them\n",
    "filename = 'solution'  # Replace with your image fqilename without extension\n",
    "direction = -1  # Set to 1 for anticlockwise, -1 for clockwise\n",
    "start_from_point = 1  # Set the starting point index (1-based index)\n",
    "\n",
    "corner_dict = process_solution(filename, direction, start_from_point)\n",
    "\n",
    "print(\"Corner Dictionary:\")\n",
    "for serial, coords in corner_dict.items():\n",
    "    print(f\"{serial}: {coords}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nActual Points:\")\n",
    "actual_points = {}\n",
    "\n",
    "# Iterate through each serial and its corresponding coordinates\n",
    "for serial, coords in corner_dict.items():\n",
    "    # Extract the coordinates from the dictionary\n",
    "    point_in_camera = coords\n",
    "    # Transform the coordinates from camera to robot frame\n",
    "    point_in_robot = camera_to_robot_coordinates(O, c, theta, point_in_camera)\n",
    "    # Transformation matrix\n",
    "\n",
    "    # transformation_matrix = np.array([[-1, 0],\n",
    "    #                                 [0, -1]])\n",
    "    # # Perform matrix multiplication\n",
    "    # point_in_robot = transformation_matrix @ point_in_robot\n",
    "    \n",
    "    # Convert the np.float64 values to native Python float and store them in the new dictionary\n",
    "    actual_points[serial] =  [round(float(point_in_robot[0]),2), round(float(point_in_robot[1]),2)]\n",
    "    # Prepare the string output in MATLAB containers.Map format\n",
    "\n",
    "for serial, coords in actual_points.items():\n",
    "    print(f\"{serial}= {coords}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ikpy.chain import Chain\n",
    "from scipy.optimize import fsolve, root\n",
    "import sounddevice as sd\n",
    "import sympy as sp\n",
    "import cv2\n",
    "import cv2.aruco as aruco\n",
    "import math\n",
    "import time\n",
    "import socket\n",
    "from ikpy.urdf import URDF\n",
    "from ikpy.utils import geometry\n",
    "\n",
    "\n",
    "# Mathemaatically these calcualtions can never give any error. It's always 100000000% correct.\n",
    "error_tolerance = 0.001      # Tolerance for differences in inverse and forward. (in mm)\n",
    "z= 60                      # Height of the end effector form the table top. (in mm)\n",
    "\n",
    "# Alert sound\n",
    "def urgent_buzzer(duration=0.4, freq=400, fs=44100):\n",
    "    t = np.linspace(0, duration, int(fs * duration), endpoint=False)\n",
    "    buzzer = 0.5 * np.sin(6 * np.pi * freq * t) * (np.sin(12 * np.pi * 5 * t) > 0)  # Modulated \"on-off\"\n",
    "    sd.play(buzzer, fs)\n",
    "    sd.wait()\n",
    "\n",
    "# Define the symbolic variables\n",
    "Px, Py, Pz, t4, t5, t6 = sp.symbols('Px Py Pz t4 t5 t6')\n",
    "\n",
    "# H14 Matrix\n",
    "H14 = sp.Matrix([[Px / sp.sqrt(Px**2 + Py**2),  0, -Py / sp.sqrt(Px**2 + Py**2), Px],\n",
    "                    [Py / sp.sqrt(Px**2 + Py**2),  0,  Px / sp.sqrt(Px**2 + Py**2), Py],\n",
    "                    [0, -1, 0, Pz],\n",
    "                    [0, 0, 0, 1]])\n",
    "\n",
    "# H45 Matrix\n",
    "H45 = sp.Matrix([[sp.cos(t4),  0, -sp.sin(t4), 0],\n",
    "                    [sp.sin(t4),  0,  sp.cos(t4), 0],\n",
    "                    [0, -1, 0, 219 / 2],\n",
    "                    [0, 0, 0, 1]])\n",
    "\n",
    "# H56 Matrix\n",
    "H56 = sp.Matrix([[sp.cos(t5), 0, sp.sin(t5), 0],\n",
    "                    [sp.sin(t5), 0, -sp.cos(t5), 0],\n",
    "                    [0, 1, 0, 107],\n",
    "                    [0, 0, 0, 1]])\n",
    "\n",
    "# H6e Matrix\n",
    "H6e = sp.Matrix([[sp.cos(t6), -sp.sin(t6), 0, 0],\n",
    "                    [sp.sin(t6), sp.cos(t6), 0, 0],\n",
    "                    [0, 0, 1, 381 / 5],\n",
    "                    [0, 0, 0, 1]])\n",
    "\n",
    "# Compute the total transformation matrix H_total\n",
    "H_total = H14 * H45 * H56 * H6e\n",
    "\n",
    "translation_vector = H_total[0:3, 3]\n",
    "\n",
    "Sx = translation_vector[0]\n",
    "Sy = translation_vector[1]\n",
    "Sz = translation_vector[2]\n",
    "        \n",
    "# Extract the translational vector (last column of H_total)\n",
    "translation_vector = H_total[0:3, 3]\n",
    "\n",
    "\n",
    "def get_link3endsimple(Kx_val, Ky_val, Kz_val, t4_val=-90, t5_val=-90):  \n",
    "    # t4_val this is fake angle when we consider the robot to be link1 connected to link 4 directly.\n",
    "    \n",
    "    # Define the symbolic variables\n",
    "    Px, Py, Pz, t4, t5, t6 = sp.symbols('Px Py Pz t4 t5 t6')\n",
    "\n",
    "    # Convert angles t4, t5 to radians\n",
    "    t4_val = t4_val * sp.pi / 180  # Convert to radians\n",
    "    t5_val = t5_val * sp.pi / 180  # Convert to radians\n",
    "\n",
    "    # Set up the system of equations\n",
    "    eq1 = sp.Eq(Sx, Kx_val)\n",
    "    eq2 = sp.Eq(Sy, Ky_val)\n",
    "    eq3 = sp.Eq(Sz, Kz_val)\n",
    "\n",
    "    # Substitute the t4, t5 values into the equations\n",
    "    eq1_subs = eq1.subs({t4: t4_val, t5: t5_val})\n",
    "    eq2_subs = eq2.subs({t4: t4_val, t5: t5_val})\n",
    "    eq3_subs = eq3.subs({t4: t4_val, t5: t5_val})\n",
    "\n",
    "    # Set initial guesses for Px, Py, Pz\n",
    "    initial_guess = (100, 100, 100)  # Initial guess for (Px, Py, Pz)\n",
    "\n",
    "    # Use numerical solving (nsolve) for all three equations with initial guesses\n",
    "    solution = sp.nsolve([eq1_subs, eq2_subs, eq3_subs], (Px, Py, Pz), initial_guess)\n",
    "\n",
    "    # Convert the solution to numerical values using evalf\n",
    "    Px = float(solution[0])  # Convert to numeric (float)\n",
    "    Py = float(solution[1])\n",
    "    Pz = float(solution[2])\n",
    "\n",
    "    # Return the solved values as numeric\n",
    "    return Px, Py, Pz\n",
    "\n",
    "\n",
    "\n",
    "# This then calcualtes all teh joint angles.\n",
    "def get_inverse(x, y, z=133.8, t4_val=-90, t5_val=-90):\n",
    "    \n",
    "    Px, Py, Pz = get_link3endsimple(x, y, z, t4_val, t5_val)\n",
    "    #print(Px,Py,Pz)\n",
    "\n",
    "    t1 = np.arctan2(Py, Px) * 180 / np.pi\n",
    "    dz = Pz-210 # 210 is link1 length, we need to be on the link1 heihgt plane to solve triangle base and extra_t2\n",
    "    # The base for triangle formed by links 2 and 3\n",
    "    base = np.sqrt(Px**2 + Py**2 + (dz)**2)\n",
    "\n",
    "    base2 = np.sqrt(Px**2 + Py**2) # Base with no z componenet, it helps to know the extraa_t2 which is made by sides base2 and Pz.\n",
    "    #print(base2)\n",
    "    #print(Pz-210)\n",
    "    extra_t2 = np.degrees(np.arctan2(dz, base2))\n",
    "    #print(extra_t2)\n",
    "\n",
    "    # Step 2: Use the Law of Cosines to find the angle at the vertex (theta)\n",
    "    side = 250  # Length of the equal sides (250)\n",
    "    theta = np.arccos((2 * side**2 - base**2) / (2 * side**2))  # Angle in radians\n",
    "\n",
    "    alpha_actual = np.degrees((np.pi - theta)/2)\n",
    "    #print(\"alpha actual: \", alpha_actual)\n",
    "    # Base angles (alpha) for link 2 and 3\n",
    "\n",
    "    # Convert to degrees\n",
    "    theta_deg = np.degrees(theta)\n",
    "\n",
    "    angles = [\n",
    "        t1,\n",
    "        -1 * (alpha_actual+extra_t2),\n",
    "        180 - theta_deg,\n",
    "        -1 * (alpha_actual-extra_t2)+t4_val,\n",
    "        t5_val,\n",
    "        0\n",
    "    ]\n",
    "    if any(np.isnan(angle) for angle in angles): print(\"NAAAAAAAAAAAAAAAAAANNNNNNNNN\"); urgent_buzzer()\n",
    "\n",
    "    if (180 - theta_deg) > 149:\n",
    "        urgent_buzzer()\n",
    "\n",
    "    robot_chain = Chain.from_urdf_file(\"my_pro600.urdf\")\n",
    "    new_angles = [0.0] + angles\n",
    "    angles_rad = np.radians(new_angles)\n",
    "    fk_result_1 = robot_chain.forward_kinematics(angles_rad)\n",
    "    fk_result_1_rounded = np.round(fk_result_1, 8)\n",
    "    print(fk_result_1_rounded)\n",
    "\n",
    "    target_position = np.array([x, y , z])\n",
    "    diff_x = round((fk_result_1_rounded[0, 3]*1000 - target_position[0]), 3)\n",
    "    diff_y = round((fk_result_1_rounded[1, 3]*1000 - target_position[1]), 3)\n",
    "    diff_z = round((fk_result_1_rounded[2, 3]*1000 - target_position[2]), 3)\n",
    "    if abs(diff_x) > error_tolerance or abs(diff_y) > error_tolerance or abs(diff_z) > error_tolerance:\n",
    "        print(\"\\n\\033[1;34m********************BADDDDD********************\\033[0m\\n\")\n",
    "        urgent_buzzer()\n",
    "    print(f\"Differences for solution 1: \\033[1;31mDx = {diff_x}, Dy = {diff_y}, Dz = {diff_z}\\033[0m\")\n",
    "\n",
    "    return angles\n",
    "\n",
    "\n",
    "\n",
    "joint_angles_dict = {}\n",
    " \n",
    "# Convert dictionary to a list of items for iteration with index\n",
    "actual_points_items = list(actual_points.items())\n",
    "\n",
    "# Iterate over the dictionary, skipping the first and last items\n",
    "for idx, (serial, coords) in enumerate(actual_points_items):\n",
    "    if idx == 0 :\n",
    "        continue  # Skip the first and last items\n",
    "\n",
    "    point_in_robot = coords\n",
    "    x, y = point_in_robot\n",
    "\n",
    "    print(f\"\\n{serial} Coordinates: \\033[1;93m({round(x,3)}, {round(y,3)}, {round(z,3)})\\033[0m\")\n",
    "    #print(f\"Distance: {(Px**2 + Py**2)**0.5}\")\n",
    "\n",
    "    # Call your inverse kinematics function\n",
    "    joint_angles = get_inverse(x, y, z)\n",
    "    joint_angles_dict[serial] = joint_angles  # Store the joint angles under the serial number\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "for serial, joint_angles in joint_angles_dict.items():\n",
    "    print(f\"{serial}: {joint_angles}\")\n",
    "    i=1\n",
    "    for _ in joint_angles:\n",
    "        print(f\"t{i}v= {_};\")\n",
    "        i+=1\n",
    "    print(\"/n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_inverse(x=0, y=350, z=80, t4_val=45, t5_val=-90)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send Angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "\n",
    "# Define your server IP and port\n",
    "SERVER_IP = \"192.168.1.159\"\n",
    "SERVER_PORT = 5001\n",
    "\n",
    "# Sound alert\n",
    "def urgent_buzzer(duration=0.64, freq=600, fs=44100):\n",
    "    t = np.linspace(0, duration, int(fs * duration), endpoint=False)\n",
    "    buzzer = 0.5 * np.sin(2 * np.pi * freq * t) * (np.sin(4 * np.pi * 4 * t) > 0)  # Modulated \"on-off\"\n",
    "    sd.play(buzzer, fs)\n",
    "    sd.wait()\n",
    "\n",
    "# Function to send a TCP packet to the robot\n",
    "def send_tcp_packet(server_ip, server_port, message):\n",
    "    try:\n",
    "        # Create a TCP socket\n",
    "        client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "        # Connect to the server\n",
    "        client_socket.connect((server_ip, server_port))\n",
    "        print(f\"Connected to {server_ip}:{server_port}\")\n",
    "\n",
    "        # Send the message\n",
    "        client_socket.sendall(message.encode('utf-8'))\n",
    "        print(f\"Sent: {message}\")\n",
    "\n",
    "        # Optionally receive a response (if server sends one)\n",
    "        response = client_socket.recv(1024).decode('utf-8')\n",
    "        print(f\"Received: {response}\")\n",
    "\n",
    "    except socket.error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    finally:\n",
    "        # Close the connection\n",
    "        client_socket.close()\n",
    "        #print(\"Connection closed.\")\n",
    "\n",
    "# Function to send the joint angles\n",
    "def send_point_angles(solutions):\n",
    "\n",
    "    i=1\n",
    "    for angles in solutions:\n",
    "        # Format angles as a string with 2 decimal places for TCP message\n",
    "        rounded_angles = [round(angle, 2) for angle in angles]\n",
    "        print(rounded_angles)\n",
    "        angle_message = f\"set_angles({', '.join(map(str, rounded_angles))}, 1000)\"\n",
    "\n",
    "        # Send the message to the robot\n",
    "        send_tcp_packet(SERVER_IP, SERVER_PORT, angle_message)\n",
    "        if i==1:\n",
    "            print(\"Waiting 2s before starting the MAZE!\\n\")\n",
    "            time.sleep(5)\n",
    "        i+=1\n",
    "        time.sleep(0)\n",
    "\n",
    "# Sending angles for each coordinate:\n",
    "urgent_buzzer()\n",
    "time.sleep(1)\n",
    "send_point_angles(solutions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myRobenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
